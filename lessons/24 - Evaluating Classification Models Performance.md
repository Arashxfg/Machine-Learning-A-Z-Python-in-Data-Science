# False Positives False Negatives

&nbsp;&nbsp;&nbsp;As you remember where we were using the logistic regression function to observe where for random values of the independent variable will end up in terms of y had so in terms of the predicted value for the dependent variable and we agreed that anything below the 50 percent line will be projected downwards onto the zero horizontal line and anything above the 50 percent line will be projected upwards onto the 100 percent horizontal line and that allowed us to turn probabilities into actual predictions. So either yes or no.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_1.PNG)

&nbsp;&nbsp;&nbsp;Now let's take a step back where did we get these four values from. So we took four random values of the independent variable and we just had a look at what would happen to them how we can use how we would use the logistic regression function to ascertain what probability they have and what y had values have.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_2.PNG)

&nbsp;&nbsp;&nbsp;So we take another step back and we forget about these four random values and instead of taking four random values of the independent variable about we take for unknown values. In fact let's take four values for the independent variable. From our data set. So let's just pick out four values that we really know they exist in our data set. And we use them to create this logistic regression. And let's do the same thing with them let's see where they will end up. If we apply the model to them. And as you can see here the label of the vertical axis change too. Why?  
Because this is the we already know that in red is the actual value of the dependent variable because we know the result. Those are the people on the bottom so observation is number one and number three they didn't take up the offer the email offer. And the observations on the top people number two and four they did take up the email offer. So let's see what happens to them if we apply our logistic regression model. So step number one would be to project these values onto the curve makes sense right. So we just want to see where they all end up on the curve that's all blue dots over there that's where they have been modeled by the curve. Now we can and from here we can say what the probabilities are you just have to project to the left and you can see approximately that for a version number one. It's about maybe 20 percent 10 15 percent may say 15 percent observation or two. It's about 40 percent absorption of three I would say about 70 percent observational before about 85 percent. 

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_3.PNG)

&nbsp;&nbsp;&nbsp;But we're not interested in probabilities per se. Right now what we want to get to is the actual y hat we want to see what the predictive value will be so we wanted to say we want to see if the model will tell us are these people going to take up off or not. And why would we want to do this is because we already know the result. Right. We already know what the result will be or was and we just want to see we want to kind of assess the model we want to see how well it's working and if it's going to make any mistakes. So let's go ahead and proceed with our logic for getting the y hat and what was the logic there. Well the same thing that we discussed just a few minutes ago at the site of the Statoil anything we're using this arbitrary horizontal line 50 percent. So anything below this line is going to be projected onto the horizontal line which is zero. So where we're saying that the offer is not going to be taken up and anything above the 50 percent line will be projected onto the horizontal line which is 1 or 100 percent. Where is saying that those people that end up on that line are different going to take up the offer. So let's go ahead and do that. There we go in gray We have our projections or our predicted values so white hat isn't great and it's very interesting to see both why and why had on one chart so that means what actually happened is in red and what we predicted was going to happen isn't great. And right away you can see that for absorptions number one and number four. So for those people in observation the morning before they we predicted correctly so we said for Person number one we predicted that she won't take up the offer. And he actually did not take up the offer because the red mark is also on the same horizontal line. Now for observation and before. Same thing we predicted that that person will will take up the offer. And they did take up the offer. That's good but now let's have a look at the rest of them. Number three you can see that for observation number 2 the gray lines at the bottom. The green marks at the bottom meaning that the model is predicting for this person based on their gender based on their age. Well in this case just age because we're doing a single variable logistic regression. So based on their age this model is saying that this person is not going to take up the offer because the gray market is at the bottom. However we can see that the red mark is at the top meaning that this person did take up the offer and that means that the logistic regression made an error here. And same thing for person number three. The gray market is at the top and that means that the models predicting that the person will will take up the offer but the red marks at the bottom meaning that the person didn't actually take up the offer and therefore the logistic regression made a mistake once again and these mistakes they actually have specific names. So the top mistake over there is a false positive or a type 1 error. What does post false positive mean. Well it means that we said we predicted a positive outcome but it was false. So we were. We predicted an effect that did not occur. And the other mistake you see here has a different name is called a false negative. So we predicted that there won't be an effect but the effect actually did occur. So our prediction was negative meaning there won't be an effect but it was a false negative and it's called a type 2 type of error. And the way I personally remember them is it's important also to distinguish between the two. The way I personally remember them is I think of type 1 as less dangerous than type 2 so type type 1 is less for me in my in my mind although it's not necessarily the case. But the way that's the way I remember them that type 1 is kind of like a warning. So that's why there's an ordered orange explanation mark and it's it's a false positive. So basically you said something is going to happen but it didn't happen.So you said maybe will be an earthquake but there wasn't an earthquake. So you know that's not the end of the world. But false negative is a bit worse in my view once again understanding because once you say something's not going to happen but it actually does happen then you can't even be prepared for it. And that's why Type 2 is false negative and that's how I remember them personally. But once again it could be. 

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_4.PNG)





# Accuracy Paradox

&nbsp;&nbsp;&nbsp;Here I've got a confusion matrix and this confusion matrix has 10000 records in it and it represents scenario number one which we'll be looking at as you can see this model has made 150 type 1 errors and 50 type 2 errors. And but overall it's predicted quite a lot correctly. Now let's calculate the accuracy rate in this scenario the accuracy rate is the total correct word. But overall total and it's 9800 divided by 10000 which is 90 percent. 

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_5.PNG)

&nbsp;&nbsp;&nbsp;Okay great but what are we going to do now is we're going to tell the model to stop making predictions whatsoever which is going to abandon them all completely. And we're going to say that from now on our prediction is always zero. We're always going to predict that the event is not going to occur. So basically what will happen to the confusion matrix is these records will move from the right call them to the left column and our new confusion matrix will look like this. Nine thousand eight hundred fifty 150 and then nothing in the predicted column where we predicted that something will occur. And of course that moves against all logic right. Why would you abandon a model but let's calculate the accuracy rate in this scenario. So in order to accuracy rate has the same formula in this case accuracy rate is nine thousand eight hundred fifty divided by 10000. So it's ninety eight and a half percent the accuracy rate went up by half a percent. And as you can see what we did is we just completely stopped using a model but the accuracy rate went up. And that is why you should not base your judgment just on accuracy right because things like this can happen and even though obviously you're not using a model any more which means that you're not applying any kind of logic into your decision making process your accuracy rate is going up so it's misleading you into a wrong conclusion that you should stop using models and this effect is called the accuracy paradox.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_6.PNG)




# CAP Curve

&nbsp;&nbsp;&nbsp;Hopefully now you see why we need more robust methods to assess how models and we talk about the cumulative accuracy profile which is in fact one of those methods. Let's look at a scenario :  
Let's say you're a scientist at a store which sells clothes and your store has a total of 100000 customers. I'm placing that number on the horizontal axis. And you know that from experience whenever you send an offer like an e-mail to all your customers or to any random sample of your customers approximately 10 percent of them respond and purchase the product. So I'm going to place 10000 which is 10 percent of the total on the vertical axis. And so what we're going to do is where we've got an offer that we want to send and we want to see how many customers are going to purchase our product. We send it off so if we send it to zero customers obviously will get zero responses right. What do you think will happen if we send it to 20000 customers. How many do you think will respond. Well because this is a random sample and we know that about 10 percent response I would say about 2000 would respond. Fair enough right. If 40000 if we send to the offer to 40000 of our customers then about 4000 will respond. Sixty thousand six thousand 80000 8000 100000 than 10000 of our customers should respond. 

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_7.PNG)

&nbsp;&nbsp;&nbsp;And this is a random selection process so here we can draw a line which will actually represent this random selection the slope of the line equals to that 10 percent that we know that respond on average to offer as if we just send them out like that. Now the question is can we somehow improve this experience can we get more customers to respond to offers when we send out our letters so basically Can we somehow target our customers more appropriately so to get a better response rate. And how about instead of sending out these offers randomly to say a random sample of 20000 customers. How would we pick and choose the customers we send these offers to and how do we pick and choose. Well to start off with let's build a model just like we did in the previous section. Basically a customer segmentation model demographic segmentation model but which wants to predict whether or not they will leave the company it will actually predict whether or not they will purchase the product. It's a very simple process actually. In fact it's the same thing because purchased is also a binary variable yes or no. And we can also run the same experiment. We can take a group of customers before we send out the offer and then look back and see who purchased with a male or female. Which country were they in what age predominantly were they were they browsing on mobile were they browsing via a computer and all of these factors we can take them into account measure them put them into a logistic regression and get a model which will help us assess the likelihood of certain types of customers purchasing based on their characteristics or the general demographic status and and other characteristics. And once we've built this model how about we apply it to select customers we will send the offer to. So what the model will tell us like just like in the example in the previous section where females of female customers of a bank whose favorite color is red they're most likely to leave the bag here will have a similar result. Ill say perhaps male customers in this certain age group who browse and mobile are most likely to purchase a product. It will tell us something or ill actually rank our customers we'll give them a probability of purchasing our product and then we can use that probability to actually contact our customers.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_8.PNG)

&nbsp;&nbsp;&nbsp;So of course if we contact zero customers will get a response rate. Then if we contact 20000 we'll probably get a much higher response rate than just 2000 because we're picking out the customers that are at the highest risk of accepting this offer. We know from their previous behavior from the previous behavior of customers similar to them that they have a 90 percent chance or an 80 percent chance of purchasing this product and we will go for them first. We will put them at the top of our list of people who we contact. Then when we contact let's say we contact not 20000 but 40000. Our response rate will be higher than 4000 which we get in the random scenario if we if our model is really good then by the time we're at around around 60 thousand so more that just over half of our total customer base we are really getting to that 10000 mark so we know that 10000 people will respond in total. There's no way we can get above that because that's just the response rate if we contact everybody it'll be 10000 but we're getting very close already. So even at 60000 we're already at nine and a half thousand respondents or purchases. We we could actually stop here. We've already pretty much contacted everyone but if we want to contact more if we send it out to 80000 we're getting even closer to 10000 responses. And if we contact 100000 we will still be back at our 10000 responses.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_9.PNG)

&nbsp;&nbsp;&nbsp;So not this draw a line through these crosses. So what you see this line here is called the cumulative accuracy profile of your model. And as you can imagine the better your model the larger will be the the area under the slides so the area between the red and the blue lines. It will increase as your model gets better and if your model is worse then this red line will be closer to the blue line so it will be closer to random. The next step we want to do is convert these axes from absolute values to percentages.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_10.PNG)

&nbsp;&nbsp;&nbsp;So they range from zero to 100 percent. And this is how the cap curve is normally represented. Now let's say we ran another regression model and this time we used less variables listen dependent variables or just because we had less access to independent variables or we didn't see that there's a multicollinearity effect in a model or something else that went wrong.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_11.PNG)

&nbsp;&nbsp;&nbsp;And that model because it will be worse. This is what it's kept curve look like. And therefore by plotting the cap curves you'll be able to compare models to each other and understand how much gain this is also sometimes called the gain chart. How much gain you get in each of these models compared to the random scenario or how much again you get additional gain you get from switching from one model to the next or from the green one to the red one for instance you're improving your hit ratio and therefore you're improving your return on investment. So therefore the read model is better. And this is how we are going to be assessing models. So let's label them the blue line is a random selection process like a monkey could do that to just pick out random sample and you send the letter or just send it to everybody you get 100 percent of respondents. The green line is a poor model. So it's always is better than random but it's still not as good as the red one the red one is a good model as you can see here or around the 50 percent mark we're getting just over 80 percent response's that's considered a good model.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_12.PNG)

&nbsp;&nbsp;&nbsp;And there's one more line that you can think of here and it's this line this line is the ideal line and this is what would happen if you had a crystal ball if you could predict exactly who is going to purchase and contact those people. This is what it would look like. Why?  
Well because if you look at that the place where that spline occurs you will see that it's exactly 10 percent and 10 percent as you remember. We know that only 10 percent of our customers ever purchase. So basically you're saying that on the horizontal axis I'm going to take 10 percent and each and every single one of those customers I pick and that 10 percent they are going to be those that purchase. That means I will go right straight to 100 percent with this last scenario. This actually took me a while to get my head around. When I first heard about it because I never understood why this line at the top. Why does it break like that. You can imagine that you have a crystal ball and you contact in the first 10 percent or however many in your specific business scenario customers ever purchase you contact them right away. And then it's just flat from there because it doesn't matter how many more you contact they're not going to purchase. That's just the reality of things. And that is the curves that you have on a cap curve. If you ever see a model that goes under the blue line. I didn't even draw one here. But if that happens that's a very bad model is basically doing you a disservice if it's if you see the curve in the blue line and we'll talk about model deterioration further in the course when we're talking about maintaining a models. So that's it for the cap curve for the introduction to cap curve will be using the cap curve very actively in this section to assess our model. And in fact we actually build two of them and one for a model and one for our test data so that will be very interesting to compare.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_13.PNG)

&nbsp;&nbsp;&nbsp;One last thing I wanted to mention is and note that we have the cap which is a cumulative accuracy profile and we have a rock which is a receiver operating characteristic. A lot of people get these things confused and my myself included I used to get it confused I even tried proving one time a colleague of mine who knew this stuff really well at the time when I was just learning it that she was wrong but that was a funny experience but they're not the same thing. So cumulative accuracy profiles all we talked about receive operating characteristics. We won't be covering this course. It'll be in my advanced course on statistics. It's very similar it looks similar and that's why a lot of people get confused and actually I think the other reason is that the rock curve is in Wikipedia there's an article for their occurred but there isn't one in English for the cumulative accuracy profile. So it's quite hard to find information on the cap curve just by searching in Google so maybe you'll be the first person to write a Wikipedia article on the cap curve.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_14.PNG)





# CAP Curve Analysis

&nbsp;&nbsp;&nbsp;There are three lines that are important on the CAP curve of the blue line which is the random line when you select your samples at random. The red line which is our model line the and different models will have different red lines but basically look something like that. And the gray line which is the perfect model or when you have a crystal ball when you can select all of the future turners or purchaser's or whatever action takers and you can select them right away on the dot before even selecting one single person that you don't want to select. And so these are the three main lines and how do we analyze this cap Kaveri know how to build it. But what can we derive what insights can we derive from here. Well it's kind of intuitive that the closer your red line is to the gray line the better you model the closer to the blue line the worse. So how can we quantify this effect.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_15.PNG)

&nbsp;&nbsp;&nbsp;Well there is a standard approach to calculate the accuracy ratio and to calculate accuracy ratios you take the area under the perfect model or the perfect line which is color in gray here and it's called a_p.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_16.PNG)

&nbsp;&nbsp;&nbsp;And then you need to take that area under the red line which is colored in red here which is a R and then you need to divide one by the other. So you need to divide a r by AP and then this ratio that you get is obviously between 0 and 1. And the closer this ratio is to 1 the better the further it is away from one and close to zero the worse. However it can be quite complicated to calculate this area under the curve statistical tools can do it for you. But how can you assess the cap curve by just looking at it so visually it's not that easy to get this quantifiable metric just by looking at the curve. 

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_17.PNG)

&nbsp;&nbsp;&nbsp;So there's a second approach and that's what we're going to discuss. Now let's get rid of areas and instead of looking at the area what you can do is look at the 50 percent line on the horizontal axis and look where it crosses your model and then look at where that line the horizontal line from there crosses the vertical axis So basically how many turners will you pick up or action takers or how many positive outcomes are you going to identify if you take 50 percent of your population. And in this case we can see it's around 90 percent or something like that. And just by looking at that there is like a rule of thumb how you can assess your model based on that X number and here it is.  
So if x is less than 60 percent the model is rubbish. Basically it's not useful at all. You can create a better one probably you can create a better one and you need to try again. If your model your X is between 60 percent and 70 percent then the model is considered to be poor poor or average and by the way these are my this is my rule of thumb. Other people might have a different rule of thumb but this is what I go by is between 60 percent and those and it's it's a poor model to be honest like you can. You can do better than that. If it's if X is between 70 percent and 80 percent that's a good model that's already where you should be aiming for anything above 70 percent that's can deliver good quality insights to the business and actually deliver value. Anything between 80 percent and 90 percent like we see here is very good. It's extremely good. That's if you can get a model over 80 percent that is an amazing result and anything above 90 percent up to 100. That is just too good. It is too good to believe and the are there is one option that there should be very careful here with his overfitting. If your model is showing you results like 90 percent to 100 percent then the obvious answer there is that one of your independent variables is actually a post facto variable meaning that it shouldn't be in the data because it's looking into the future. The person who supplied you that variable forgot to take it out or forgot to explain to you that you know their credit score actually is turned into zero after they leave the bank and therefore everybody offer zero credit card obviously has left the bank and therefore your model is picking them up like like it's super easy. So if you have 100 percent that's definitely something or if your variables even if you have 90 to 100 percent you have to check that there could be some forward looking variables. The other thing is overfitting you could be overfitting a model and what that means is that you your model has been so well fit that specific data set that you supplied it that when you try it that it's just heavily relying on the anomalies in that data set. And when you feed it a new data set like you know in a month time or something like not not training or not the data that you trained your model on. We'll talk about this a bit a lot more actually in the coming tutorials but so if you feed this model some data that you want to actually predict on then you will crash it all it won't crash it will it won't perform as well perform you know at the 60 percent mark or something. So that means the model is overfit it and be very careful about that.

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_18.PNG)



# Conclusion of Part 3 Classification

&nbsp;&nbsp;&nbsp;What are the pros and cons of each model ?  

![](../Assets/photos/Evaluating%20Classification%20Models%20Performance_19.PNG)

&nbsp;&nbsp;&nbsp;How do I know which model to choose for my problem ?  

If your problem is linear, you should go for Logistic Regression or SVM.
If your problem is non linear, you should go for K-NN, Naive Bayes, Decision Tree or Random Forest.

&nbsp;&nbsp;&nbsp;Then from a business point of view, you would rather use:

- Logistic Regression or Naive Bayes when you want to rank your predictions by their probability. For example if you want to rank your customers from the highest probability that they buy a certain product, to the lowest probability. Eventually that allows you to target your marketing campaigns. And of course for this type of business problem, you should use Logistic Regression if your problem is linear, and Naive Bayes if your problem is non linear.

- SVM when you want to predict to which segment your customers belong to. Segments can be any kind of segments, for example some market segments you identified earlier with clustering.

- Decision Tree when you want to have clear interpretation of your model results,

- Random Forest when you are just looking for high performance with less need for interpretation. 


