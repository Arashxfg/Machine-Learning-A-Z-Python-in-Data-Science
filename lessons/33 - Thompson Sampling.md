# Thompson Sampling Intuition

&nbsp;&nbsp;&nbsp;-: Hello, welcome back to the course on Machine Learning. Today we've got a very interesting topic. Today we're talking about Thompson sampling and the algorithm's intuition. And again, we're going to be using this algorithm to solve the multi-armed bandit problem. All right, so let's get started. A quick refresher on the multi-armed bandit problem. We have several slot machines. Each one of them has a distribution behind it D1 to D5. We don't know what these distributions are and we need to start playing these machines and at the same time, figure out which one has the best distribution so that we then exploit that best distribution but it'll take us some time to figure it out. So we need to maximize our return during the process of figuring out. So we have to find that ideal balance or trade off between exploration and exploitation. We had a few tutorials previously on these things. So we talked about the multi-armed bandit problem in a lot of details. So if you haven't watched that tutorial highly recommend jumping into the previous section and watching it there. Also, understanding of the upper confidence bound algorithm will really help you grasp the concepts of Thompson sampling. So if you haven't seen the upper confidence bound tutorial then I highly recommend checking that out before proceeding with today's lecture. All right, so let's get started.

![](../Assets/photos/Thompson%20Sampling_1.PNG)

&nbsp;&nbsp;&nbsp;A quick summary of the multi-armed bandit problem is as follows. We have D arms, for example. Arms are ads that we display to users each time they connect to webpage. So by the way, yes, indeed a modern application of the or modern representation of a multi-armed bandit problem is advertising. So when you display ads that is very similar or the algorithms that we're gonna be applying learning can be applied to solving that problem where you're displaying ads. 

![](../Assets/photos/Thompson%20Sampling_2.PNG)

&nbsp;&nbsp;&nbsp;So if you go back here, instead of having these slot machines, each one of these is a different ad and you want to figure out which one of these ads is the best performing ad, but you don't have time to do an AB test. So you don't have the funds or the resources to do an AB test. You want to figure it out on the fly. That's when you would apply one of these algorithms that we talking about in this part of the course. And of course, there's of other, lots of other modern problems that are very similar to the multi-armed bandit problem, therefore these algorithms are valid for them too.

![](../Assets/photos/Thompson%20Sampling_1.PNG)

&nbsp;&nbsp;&nbsp;All right, so moving back here. So we've got DR for example, arms, our ads that we display users each time they connect to a webpage each time a user connects to this page that is considered as a round at each round, and we choose which ad display to the user at each round. And the ad gives us a reward, either 01 if the ad is clicked, zero, if the ad is not clicked. In the case of the actual bandits it'll be a monetary reward. So it won't be just zero to one. It'll be some value. Our goal is to maximize the total reward we get over many rounds. All right, so that's a quick overview of the problem that we're solving. 

![](../Assets/photos/Thompson%20Sampling_2.PNG)

&nbsp;&nbsp;&nbsp;Then here we've got some very complex looking mathematics and Bayesian inference and all these distributions and posterior probability and other prior distributions and beta distributions and so on. We're not gonna delve deep into this right now. So Harlan will take some time to walk through this slide with you in the practical tutorials because we're going to be coding this from scratch. Well at least in R and therefore he will actually walk you through this slide. Our goal for today is to understand the intuition behind all of these things. So we're going to skip the slide and get to the intuition part.

![](../Assets/photos/Thompson%20Sampling_3.PNG)

&nbsp;&nbsp;&nbsp;And this is the actual steps that you're going to be using in the practical tutorial. So again, Harlan will walk you through this slide as well.

![](../Assets/photos/Thompson%20Sampling_4.PNG)

&nbsp;&nbsp;&nbsp;And today we're talking about the intuition. So this is going to be fun. This is some interesting slides coming up and get ready for a fun ride. So grab your cup of coffee, a cup of tea and even a popcorn, and let's get started. All right, so here we've got a scale y or the horizontal access is the return the return that we expect to get from a bandit. So we're going to look at a simplified problem. We're gonna look at rather than five or even 10 we're gonna just look at three bandits because it's gonna be a lot of stuff going on on this chart and I don't want to clutter it up. And we want to keep it as simple as possible just so that we can understand the concept. And then the same thing applies for five or 10 or however many machines that you'd be looking at. So here we've got the return and these vertical lines, just like in the case of the upper confidence bond we had the horizontal lines. These vertical lines represent the expected return for each of the machines. So each of the machines out of the three machines that we have, each of the machines has a distribution behind it so that the result, the amount of money that you win per a game is picked as a random value from that distribution. And we're not gonna draw distributions, but basically just imagine a distribution behind each one of these expected values. So this is just the center of that distribution or the actual expected return from that machine. And we're just visualizing it here. But this is us kind of like looking into the actual machine itself or like pulling it apart and knowing how it works or being the designer of the machine. In reality, when you're playing these machines of course you don't know this. So this is some additional information that will guide us, that will help us understand how the algorithm actually works. The algorithm itself doesn't know this information, right? So this is hidden, but it's just there for us so that we can better understand what's going on. So these are the expected returns the actual expected returns of each of the machines. And obviously if you knew this right away you would say that machine number three or the yellow machine is the best one because it has the highest return, right? It has the highest success return. You'd always just bet on this one, but again you don't know this yet. Alright, so what happens with this algorithm? Well, at the start, just like with upper confidence bond algorithm you don't know anything, right? You have no prior knowledge of the current situation or status of things and therefore all the machines are identical to you and you have to have at least one or even a couple of trial rounds just to get some data to analyze.

![](../Assets/photos/Thompson%20Sampling_5.PNG)

&nbsp;&nbsp;&nbsp;And so let's say that has happened. There's some trial rounds for machine number one or the blue machine. And based on those trial runs, the algorithm the Thompson sampling algorithm, this is where it starts getting different to the upper conference bound will construct a distribution, right? So we'll get to this distribution in a second, what means. But for now let's just do the same thing for machine, the green machine. And so again, we are just pulling the arm several times like four times for instance, and we're getting some values and that are gonna be somewhere around obviously the actual expected return, right? And based on that, and based on those values of course it's probably gonna be a bit more than four. We're constructing some sort of distribution or some sort of perception of the current state of things. And this is the part where it gets interesting. So the actual meaning of these distributions is not which you might think at first. So these distributions are actually showing us or they're representing not the distributions, we're not trying to guess the distributions behind the machine. So the first thing that might come to mind is that all right, so we've constructed distribution. And so the blue distribution is our attempt and guessing the actual distribution behind the blue machine, right? The distribution that this is the expected return for and the green is for the green machine, the yellow is for yellow machine. Well, actually not, that's not the case. We are constructing something completely different something completely out of this world. We are constructing distributions of where we think the actual expected value might lie. It's very important to understand that. So we are creating kind of a, if you want to think of it that way, we're creating an auxiliary mechanism for us to solve the problem. So we're not trying to recreate these machines we're recreating the possible way these machines could have been created kind of in that sense. So let's just solidify that, this is where we think the expected, the actual expected values will be. So let's look at the blue one for instance, we got four values and based on those four values we've constructed this distribution, which is showing us where is that value mustard. So this is mustard, the actual mustard but we don't know it, so the algorithm doesn't know it. So it's constructed a distribution trying to guess where this value is. And of course, concert say it's over here it's over there, it's over there. It's saying, okay, there's a very high likelihood it's over here, but it also could be here it could be here, it could be here. And as you move away, likelihood drops but it still could be anywhere in this blue space. Same for the thing for the green distribution. So based on the values that we've seen the random values that have been selected in the four rounds the algorithm has created this distribution, which is saying that this actual expected return from the green machine is somewhere in this area. And it's most likely to be here but it could be here, it could be here it could be here, it could be anywhere, it could be. So basically it's more likely to be here then it could be here, it could be here. And as you move away, the likelihood of it actually being there drops off. Same thing for the yellow. So it's very important to understand. So just to reiterate, we are not trying to guess the distributions behind machines, right? We are doing like kind of a little magic trick or a little hat trick. I don't know if it's called hat trick, but we're trying to do this, create this perception of the world. We're trying to mathematically explain what we think is actually going on or what could be going on. And that is also important thing because this demonstrates that the Thompson sampling is a probabilistic algorithm. The upper confidence bond was a deterministic algorithm where everything was strict, everything was okay. So whichever one has a highest upper confidence bond that's all we're going to choose and so on. But here we're creating a probabilistic perception of the world, we're saying, so it's likely to be here, but it could be anywhere in this blue area, and this one could be anywhere in this green and so on. And you'll see exactly why we've done this in the next slide we'll understand how this works and let's jump straight into that. Let's understand now that this is probably the hardest part to kind of get your head around what we've created. And now that we've created it, let's see how the algorithm is going to utilize this auxiliary mechanism that we have created. 

![](../Assets/photos/Thompson%20Sampling_6.PNG)

&nbsp;&nbsp;&nbsp;All right, so let's have a look. So there are our distributions. That's where we think. So these are the actual expected returns for each of the machines, but the algorithm doesn't know them. The old algorithm has created is are these distributions where, which allow it to kind of guess where the actual distribution might lie for each of these machines. So what it's going to do is it's actually going to trigger each of these distributions. So like we're in a new round we have to choose a machine to use. So what the algorithm will do is it will go and call this distribution and it'll pull out of value out of this distribution. And let's say pull that value, then it'll pull a value out of the green distribution, let's say it pull that value out of the green distribution and let's say and then pulls a value out of yellow distribution, is that distribution, that value. And again, it's pulling them. So according to the distribution, right? So this is a distribution of values. So basically it's most likely to pull a value somewhere in this area. Then less likely in further way you go, it's more, less less and less likely. But still it can happen that you can see that, this yellow value is actually quite far from the center but it still can happen that it pulled this value out of distribution. And it can happen that it picked this green a value out of the green distribution totally can happen in the long term of course, you're gonna be picking somewhere close to the center, like over the long run. But on a one-off basis, this can totally happen. And so now it's picked these values and guess what that means? Well, what this actually means is that, we have, by doing that we have generated our own bandit configuration. So we have created this hypothetical or imaginary batch of machine, or not batch, imaginary set of machines in our own virtual world where we are saying, okay, so the expected, the actual expected return for the blue machine is this value the actual expected return for the green machine is this value. And the actual expected return for the yellow machine is this value. So we've created this this SUBCIDO OBCIDO world or hypothetical virtual reality in which we have our own three bandits and how we're going to solve this problem. And this problem is very easy to solve. It's obvious how to solve this problem. You just pick this machine, right? Because this machine has the highest expected return out of the three, and obviously just gonna go with this machine in the virtual world in the reality. And what's that means is that now we translate this result into the actual world. In the virtual hypothetical world we've selected the green machine. So in the actual world, the algorithm will also select the green machine.

![](../Assets/photos/Thompson%20Sampling_7.PNG)

&nbsp;&nbsp;&nbsp;And what that will do so it'll basically pull the lever for this machine, right? And what that'll do is actually, it will give us a value. So the machine will spit out a value but that value is going to be based on the distribution behind this machine, where this is the actual expected value of that distribution. So the value is gonna be somewhere here probably close to the actual expected value. It doesn't have to be, again, there's a distribution behind all this. It could be far away, it could be closed but let's say in this case, it's this one. So now this information this is new information to the algorithm.

![](../Assets/photos/Thompson%20Sampling_8.PNG)

&nbsp;&nbsp;&nbsp;What it's going to do is it's going to say, Ah-huh, Okay so I pulled the green lever the lever for the green machine, I got this value. So now I have to adjust my perception of the world, right? So I have a prior probability. So these are my, well, for the green machine this is my prior distribution this is where the Bayesian inference comes into play, or it's already been in play. And this is where adding to the Bayesian inference. So that's our prior distribution. Now we've got some new information. This is our new information. We're going to add it in and see how that changes our perception of the world. Well, perception of the world has changed the distribution has shifted a bit and it's become narrower because we have more information. Our sample size has increased, of course. Excuse me, of course it's not going to increase that much. This is just an example to demonstrate what we're talking about, to get the point across. But that's the point that every time we add new information our distribution becomes more and more refined. So now we have a new perception of the world.

![](../Assets/photos/Thompson%20Sampling_9.PNG)

&nbsp;&nbsp;&nbsp;Now what happens next is a new round, right? Same thing, we're gonna do the same thing again for a new round. Again, we generate or we pick some values out of our distributions that they are, now, we've constructed a or we've generated our own bandit configuration in our virtual reality or in our hypothetical world. Out of these three, we have to pick the best bandit which is of course the one here, the yellow bandit.

![](../Assets/photos/Thompson%20Sampling_10.PNG)

&nbsp;&nbsp;&nbsp;And we are going to now pull, actually pull in the real world, pull the lever of the yellow bandit, or the algorithm's gonna do that. That's gonna trigger the distribution behind the yellow bandit. And that will give us some sort of value. So this is the actual value that we received in the real world.

![](../Assets/photos/Thompson%20Sampling_11.PNG)

&nbsp;&nbsp;&nbsp;









