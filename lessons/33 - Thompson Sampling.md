# Thompson Sampling Intuition

&nbsp;&nbsp;&nbsp;-: Hello, welcome back to the course on Machine Learning. Today we've got a very interesting topic. Today we're talking about Thompson sampling and the algorithm's intuition. And again, we're going to be using this algorithm to solve the multi-armed bandit problem. All right, so let's get started. A quick refresher on the multi-armed bandit problem. We have several slot machines. Each one of them has a distribution behind it D1 to D5. We don't know what these distributions are and we need to start playing these machines and at the same time, figure out which one has the best distribution so that we then exploit that best distribution but it'll take us some time to figure it out. So we need to maximize our return during the process of figuring out. So we have to find that ideal balance or trade off between exploration and exploitation. We had a few tutorials previously on these things. So we talked about the multi-armed bandit problem in a lot of details. So if you haven't watched that tutorial highly recommend jumping into the previous section and watching it there. Also, understanding of the upper confidence bound algorithm will really help you grasp the concepts of Thompson sampling. So if you haven't seen the upper confidence bound tutorial then I highly recommend checking that out before proceeding with today's lecture. All right, so let's get started.

![](../Assets/photos/Thompson%20Sampling_1.PNG)

&nbsp;&nbsp;&nbsp;A quick summary of the multi-armed bandit problem is as follows. We have D arms, for example. Arms are ads that we display to users each time they connect to webpage. So by the way, yes, indeed a modern application of the or modern representation of a multi-armed bandit problem is advertising. So when you display ads that is very similar or the algorithms that we're gonna be applying learning can be applied to solving that problem where you're displaying ads. 

![](../Assets/photos/Thompson%20Sampling_2.PNG)

&nbsp;&nbsp;&nbsp;So if you go back here, instead of having these slot machines, each one of these is a different ad and you want to figure out which one of these ads is the best performing ad, but you don't have time to do an AB test. So you don't have the funds or the resources to do an AB test. You want to figure it out on the fly. That's when you would apply one of these algorithms that we talking about in this part of the course. And of course, there's of other, lots of other modern problems that are very similar to the multi-armed bandit problem, therefore these algorithms are valid for them too.

![](../Assets/photos/Thompson%20Sampling_1.PNG)

&nbsp;&nbsp;&nbsp;All right, so moving back here. So we've got DR for example, arms, our ads that we display users each time they connect to a webpage each time a user connects to this page that is considered as a round at each round, and we choose which ad display to the user at each round. And the ad gives us a reward, either 01 if the ad is clicked, zero, if the ad is not clicked. In the case of the actual bandits it'll be a monetary reward. So it won't be just zero to one. It'll be some value. Our goal is to maximize the total reward we get over many rounds. All right, so that's a quick overview of the problem that we're solving. 

![](../Assets/photos/Thompson%20Sampling_2.PNG)

&nbsp;&nbsp;&nbsp;Then here we've got some very complex looking mathematics and Bayesian inference and all these distributions and posterior probability and other prior distributions and beta distributions and so on. We're not gonna delve deep into this right now. So Harlan will take some time to walk through this slide with you in the practical tutorials because we're going to be coding this from scratch. Well at least in R and therefore he will actually walk you through this slide. Our goal for today is to understand the intuition behind all of these things. So we're going to skip the slide and get to the intuition part.

![](../Assets/photos/Thompson%20Sampling_3.PNG)

&nbsp;&nbsp;&nbsp;And this is the actual steps that you're going to be using in the practical tutorial. So again, Harlan will walk you through this slide as well.

![](../Assets/photos/Thompson%20Sampling_4.PNG)

&nbsp;&nbsp;&nbsp;And today we're talking about the intuition. So this is going to be fun. This is some interesting slides coming up and get ready for a fun ride. So grab your cup of coffee, a cup of tea and even a popcorn, and let's get started. All right, so here we've got a scale y or the horizontal access is the return the return that we expect to get from a bandit. So we're going to look at a simplified problem. We're gonna look at rather than five or even 10 we're gonna just look at three bandits because it's gonna be a lot of stuff going on on this chart and I don't want to clutter it up. And we want to keep it as simple as possible just so that we can understand the concept. And then the same thing applies for five or 10 or however many machines that you'd be looking at. So here we've got the return and these vertical lines, just like in the case of the upper confidence bond we had the horizontal lines. These vertical lines represent the expected return for each of the machines. So each of the machines out of the three machines that we have, each of the machines has a distribution behind it so that the result, the amount of money that you win per a game is picked as a random value from that distribution. And we're not gonna draw distributions, but basically just imagine a distribution behind each one of these expected values. So this is just the center of that distribution or the actual expected return from that machine. And we're just visualizing it here. But this is us kind of like looking into the actual machine itself or like pulling it apart and knowing how it works or being the designer of the machine. In reality, when you're playing these machines of course you don't know this. So this is some additional information that will guide us, that will help us understand how the algorithm actually works. The algorithm itself doesn't know this information, right? So this is hidden, but it's just there for us so that we can better understand what's going on. So these are the expected returns the actual expected returns of each of the machines. And obviously if you knew this right away you would say that machine number three or the yellow machine is the best one because it has the highest return, right? It has the highest success return. You'd always just bet on this one, but again you don't know this yet. Alright, so what happens with this algorithm? Well, at the start, just like with upper confidence bond algorithm you don't know anything, right? You have no prior knowledge of the current situation or status of things and therefore all the machines are identical to you and you have to have at least one or even a couple of trial rounds just to get some data to analyze.

![](../Assets/photos/Thompson%20Sampling_5.PNG)

&nbsp;&nbsp;&nbsp;And so let's say that has happened. There's some trial rounds for machine number one or the blue machine. And based on those trial runs, the algorithm the Thompson sampling algorithm, this is where it starts getting different to the upper conference bound will construct a distribution, right? So we'll get to this distribution in a second, what means. But for now let's just do the same thing for machine, the green machine. And so again, we are just pulling the arm several times like four times for instance, and we're getting some values and that are gonna be somewhere around obviously the actual expected return, right? And based on that, and based on those values of course it's probably gonna be a bit more than four. We're constructing some sort of distribution or some sort of perception of the current state of things. And this is the part where it gets interesting. So the actual meaning of these distributions is not which you might think at first. So these distributions are actually showing us or they're representing not the distributions, we're not trying to guess the distributions behind the machine. So the first thing that might come to mind is that all right, so we've constructed distribution. And so the blue distribution is our attempt and guessing the actual distribution behind the blue machine, right? The distribution that this is the expected return for and the green is for the green machine, the yellow is for yellow machine. Well, actually not, that's not the case. We are constructing something completely different something completely out of this world. We are constructing distributions of where we think the actual expected value might lie. It's very important to understand that. So we are creating kind of a, if you want to think of it that way, we're creating an auxiliary mechanism for us to solve the problem. So we're not trying to recreate these machines we're recreating the possible way these machines could have been created kind of in that sense. So let's just solidify that, this is where we think the expected, the actual expected values will be. So let's look at the blue one for instance, we got four values and based on those four values we've constructed this distribution, which is showing us where is that value mustard. So this is mustard, the actual mustard but we don't know it, so the algorithm doesn't know it. So it's constructed a distribution trying to guess where this value is. And of course, concert say it's over here it's over there, it's over there. It's saying, okay, there's a very high likelihood it's over here, but it also could be here it could be here, it could be here. And as you move away, likelihood drops but it still could be anywhere in this blue space. Same for the thing for the green distribution. So based on the values that we've seen the random values that have been selected in the four rounds the algorithm has created this distribution, which is saying that this actual expected return from the green machine is somewhere in this area. And it's most likely to be here but it could be here, it could be here it could be here, it could be anywhere, it could be. So basically it's more likely to be here then it could be here, it could be here. And as you move away, the likelihood of it actually being there drops off. Same thing for the yellow. So it's very important to understand. So just to reiterate, we are not trying to guess the distributions behind machines, right? We are doing like kind of a little magic trick or a little hat trick. I don't know if it's called hat trick, but we're trying to do this, create this perception of the world. We're trying to mathematically explain what we think is actually going on or what could be going on. And that is also important thing because this demonstrates that the Thompson sampling is a probabilistic algorithm. The upper confidence bond was a deterministic algorithm where everything was strict, everything was okay. So whichever one has a highest upper confidence bond that's all we're going to choose and so on. But here we're creating a probabilistic perception of the world, we're saying, so it's likely to be here, but it could be anywhere in this blue area, and this one could be anywhere in this green and so on. And you'll see exactly why we've done this in the next slide we'll understand how this works and let's jump straight into that. Let's understand now that this is probably the hardest part to kind of get your head around what we've created. And now that we've created it, let's see how the algorithm is going to utilize this auxiliary mechanism that we have created. 

![](../Assets/photos/Thompson%20Sampling_6.PNG)

&nbsp;&nbsp;&nbsp;All right, so let's have a look. So there are our distributions. That's where we think. So these are the actual expected returns for each of the machines, but the algorithm doesn't know them. The old algorithm has created is are these distributions where, which allow it to kind of guess where the actual distribution might lie for each of these machines. So what it's going to do is it's actually going to trigger each of these distributions. So like we're in a new round we have to choose a machine to use. So what the algorithm will do is it will go and call this distribution and it'll pull out of value out of this distribution. And let's say pull that value, then it'll pull a value out of the green distribution, let's say it pull that value out of the green distribution and let's say and then pulls a value out of yellow distribution, is that distribution, that value. And again, it's pulling them. So according to the distribution, right? So this is a distribution of values. So basically it's most likely to pull a value somewhere in this area. Then less likely in further way you go, it's more, less less and less likely. But still it can happen that you can see that, this yellow value is actually quite far from the center but it still can happen that it pulled this value out of distribution. And it can happen that it picked this green a value out of the green distribution totally can happen in the long term of course, you're gonna be picking somewhere close to the center, like over the long run. But on a one-off basis, this can totally happen. And so now it's picked these values and guess what that means? Well, what this actually means is that, we have, by doing that we have generated our own bandit configuration. So we have created this hypothetical or imaginary batch of machine, or not batch, imaginary set of machines in our own virtual world where we are saying, okay, so the expected, the actual expected return for the blue machine is this value the actual expected return for the green machine is this value. And the actual expected return for the yellow machine is this value. So we've created this this SUBCIDO OBCIDO world or hypothetical virtual reality in which we have our own three bandits and how we're going to solve this problem. And this problem is very easy to solve. It's obvious how to solve this problem. You just pick this machine, right? Because this machine has the highest expected return out of the three, and obviously just gonna go with this machine in the virtual world in the reality. And what's that means is that now we translate this result into the actual world. In the virtual hypothetical world we've selected the green machine. So in the actual world, the algorithm will also select the green machine.

![](../Assets/photos/Thompson%20Sampling_7.PNG)

&nbsp;&nbsp;&nbsp;And what that will do so it'll basically pull the lever for this machine, right? And what that'll do is actually, it will give us a value. So the machine will spit out a value but that value is going to be based on the distribution behind this machine, where this is the actual expected value of that distribution. So the value is gonna be somewhere here probably close to the actual expected value. It doesn't have to be, again, there's a distribution behind all this. It could be far away, it could be closed but let's say in this case, it's this one. So now this information this is new information to the algorithm.

![](../Assets/photos/Thompson%20Sampling_8.PNG)

&nbsp;&nbsp;&nbsp;What it's going to do is it's going to say, Ah-huh, Okay so I pulled the green lever the lever for the green machine, I got this value. So now I have to adjust my perception of the world, right? So I have a prior probability. So these are my, well, for the green machine this is my prior distribution this is where the Bayesian inference comes into play, or it's already been in play. And this is where adding to the Bayesian inference. So that's our prior distribution. Now we've got some new information. This is our new information. We're going to add it in and see how that changes our perception of the world. Well, perception of the world has changed the distribution has shifted a bit and it's become narrower because we have more information. Our sample size has increased, of course. Excuse me, of course it's not going to increase that much. This is just an example to demonstrate what we're talking about, to get the point across. But that's the point that every time we add new information our distribution becomes more and more refined. So now we have a new perception of the world.

![](../Assets/photos/Thompson%20Sampling_9.PNG)

&nbsp;&nbsp;&nbsp;Now what happens next is a new round, right? Same thing, we're gonna do the same thing again for a new round. Again, we generate or we pick some values out of our distributions that they are, now, we've constructed a or we've generated our own bandit configuration in our virtual reality or in our hypothetical world. Out of these three, we have to pick the best bandit which is of course the one here, the yellow bandit.

![](../Assets/photos/Thompson%20Sampling_10.PNG)

&nbsp;&nbsp;&nbsp;And we are going to now pull, actually pull in the real world, pull the lever of the yellow bandit, or the algorithm's gonna do that. That's gonna trigger the distribution behind the yellow bandit. And that will give us some sort of value. So this is the actual value that we received in the real world.

![](../Assets/photos/Thompson%20Sampling_11.PNG)

&nbsp;&nbsp;&nbsp;Now we're gonna incorporate that value into our perception of the world and our perception of the world's going to change, is gonna adjust. There we go. Now we're going to do that again.

![](../Assets/photos/Thompson%20Sampling_12.PNG)

&nbsp;&nbsp;&nbsp;All right, so let's just do this one more time just so we practice the logic behind all of this. So new round generates the bandit configuration, right? So this is what we're going to think that our expected, actual expected returns are going to be our bandit configuration. 

![](../Assets/photos/Thompson%20Sampling_13.PNG)

&nbsp;&nbsp;&nbsp;This is obviously the best one we're going to use. Pull the yellow machines or lever that's going to trigger the distribution behind the yellow machine is gonna spit out a value in the real world, there's a value.

![](../Assets/photos/Thompson%20Sampling_14.PNG)

&nbsp;&nbsp;&nbsp;And then we're going to have to adjust our perception of the world again to match the, or to incorporate the new information. 

![](../Assets/photos/Thompson%20Sampling_15.PNG)

&nbsp;&nbsp;&nbsp;And so on, we're going to keep doing that until we get to a point where we've refined the distributions substantially and the picture looks like this so they might be refined even more. This one might be more refined, this one might more refined. But as you could see from there we slowly will start generating more and more rounds based on the yellow machine will be triggering the yellow machine. So these ones might not even get that refined in the long run, which is totally fine because our point is to get to the best machine to find it and exploit it as much as we can. So there we go. That is pretty much how the Thompson sampling algorithm works. And as you can see, it is a probabilistic algorithm. And every time we're generating these values, we're kind of creating this hypothetical setup of the bandits, and then we're solving that and then we're applying the results to the real world. We're adjusting our perception of reality based on the new information that that generates. And then we're doing that again. So hopefully this was a interesting tutorial. I find this algorithm very, very cool. And in the next tutorial we're going to compare a little bit upper confidence bound and the Thompson sampling algorithm and I can't wait to see you there. Until next time, happy analyzing.

![](../Assets/photos/Thompson%20Sampling_16.PNG)




# Algorithm Comparison UCB vs Thompson Sampling

&nbsp;&nbsp;&nbsp;I hope you enjoyed the previous tutorials and now you're quite confident with the upper confidence bound and the Thompson sampling algorithm, or at least the intuition behind them, and today we're going to quickly compare the two because they do solve the same problem. They solve the problem of the multi-armed bandit, and let's have a look at some of the pros and cons of each of the algorithms. So of course there's lots and lots of different advantages and disadvantages and differences between the two, but we'll just highlight the main ones. So here we've got the two algorithms. On the left we've got the UCB and the image from the intuition tutorial that will help us kinda remember what it's about and the Thompson sampling algorithm and the same, the image behind it. So the first characteristic is probably, that is different, is that the UCB is a deterministic algorithm, and actually, there's lots of different, there's lots of different modifications to the UCB algorithm. You can find them online. There's lots of different white papers on how the UCB algorithm can be modified to improve it and to make it better at one thing, or adds one advantage, maybe makes it a bit better result, but it makes it more computationally intensive, or the other way around, and so on. But all of that, all of those algorithms belong to a family of UCB algorithms, or upper confident bound algorithms, which are all deterministic, and basically what that means is that it is very straightforward. So once you have a certain round, if it's very straightforward, what's going to happen? Or you just look at the upper confidence bound, and whichever one has the highest, that's the one you pick. You pull the algorithm, the yes, you pull the lever, yes, then you do get like a random value from the machine. But that's on the side of the machine. So that randomness is on the machine, but then, whatever value you get, it is very determined, it is very deterministic, what the UCB is going to do with that value. And so all the steps that the UCB actually takes, they are very deterministic. There's no randomness in the algorithm, itself. Whereas, on the other hand, the Thompson sampling algorithm is a probabilistic algorithm because in the algorithm itself, it has these distributions which represent our perception of the world and where we think the actual expected returns of each of those machines might lie, and therefore every time we are implementing or iterating in the Thompson sampling algorithm, we actually generate random values from those distributions. So if you rerun a round in the UCB algorithm, just one given round, after you've received the previous value from the machine, and then you rerun the round, it's always going to be the same result, whereas in the Thompson sampling algorithm, after you've received the previous value from the machine and you rerun the current round, it's always going to different because you're always sampling from your distributions which characterize your perception of the world, and that is a whole different type of algorithm. It's a probabilistic algorithm. And those two things, they actually have a few different implications. So for instance, one important one is that the UCB requires an update at every round. So basically the value that you get back from the machine, so once you've pulled the lever and you get a value back from that machine, that value, you have to incorporate it right away in order to proceed to the next round. You cannot proceed to the next round until you have incorporated that value, until you have made an adjustment to the algorithm based on that value, because if you don't make the adjustment, then nothing changes and you're going to be stuck. Whereas in the Thompson sampling, it can accommodate delayed feedback, and this is very important. This basically means that, if you pull the lever and you only will get the result or you will only know the result of pulling the lever 500 rounds down the track, not right away, you'll only get to know the result 500 rounds later, the Thompson sampling algorithm will still work. Why will it still work? Because if you now run the algorithm without even updating your perception of the world, you're still going to get a new set of hypothetical bandits. You're going to generate a new expected return for every bandit because you are generating them in a probabilistic manner. And this is very important to understand, because this gives the Thompson sampling that advantage that you don't have to update the algorithm with the result every time. And in terms of bandits, of course that doesn't really matter that much, because if you're playing in the casino and if some hypothetical person is playing in the casino and they're pulling these levers, they get to see the results right away, so they could update the algorithm. But in terms of websites and ads, that is a big deal. So not even just displaying ads on a website, or you could use this for AB testing different, instead of AB testing the different layouts of your website, you could use a Thompson sampling algorithm to have that balance between exploitation, exploration, right away. So basically anything you're doing on the web with Thompson sampling or solving a multi-armed bandit problem for your business or for a business on the web, you're getting all these thousands and tens of thousands of clicks, and to update the algorithm right away, that would be very highly computationally costly, or it might require additional resources and a complex process. Whereas what the Thompson sampling algorithm allows you to do is to update your dataset or your information in the algorithm in a batch manner. So you wait until you get 500 clicks or you wait until you get 5,000 clicks, you update the algorithm. Then you let it run, and then it runs, runs, runs, and then you get another 5,000 clicks and then you update the algorithm again, and it will still work, and that's a very important thing, that flexibility that the Thompson sampling algorithm creates. And finally, again, we're not gonna go into too much detail on the pros and cons, but the Thompson sampling algorithm is actually, it has better empirical evidence than the UCB, and you'll find this phrase, better empirical evidence, and that's because, up until recently, the theory behind the Thompson sampling algorithm, or the whole research, wasn't complete. It's only been researched in a lot of detail just a few years ago, and now you can find a lot of information on Thompson sampling algorithm. But previously people just see that, from experimental evidence, that Thompson sampling algorithm does work better than the UCB, and that's exactly what we're going to see. Spoiler alert, that's exactly what you're going to see in the practical tutorials for this section. So we're going to be now coding the, or Head Lund will walk you through coding the same exercise, the same problem that we had previously with the UCB. Now we're going to be, or you're going to be solving it with the Thompson sampling algorithm and you will see, actually, some very interesting results. We'll leave it at that, and I hope you enjoyed these intuition tutorials, and off we go to the practical side of things. Can't wait for you to get started. Head Lund will show you all around, and I'll see you back here next time. Until then, enjoy machine learning.


![](../Assets/photos/Thompson%20Sampling_17.PNG)


# Thompson Sampling in Python

![](../Assets/photos/Thompson%20Sampling_18.PNG)

![](../Assets/photos/Thompson%20Sampling_19.PNG)

![](../Assets/photos/Thompson%20Sampling_20.PNG)

![](../Assets/photos/Thompson%20Sampling_21.PNG)

![](../Assets/photos/Thompson%20Sampling_22.PNG)

![](../Assets/photos/Thompson%20Sampling_23.PNG)


